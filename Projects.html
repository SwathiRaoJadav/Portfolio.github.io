<!DOCTYPE HTML>
<!--
	Solid State by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Swathi Jadav</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Page Wrapper -->
			<div id="page-wrapper">

				<!-- Header -->
					<header id="header">
						<h1><a href="index.html">Swathi Jadav</a></h1>
						<nav>
							

							<a href="index.html">Home</a>
							<a href="index.html#About">About</a>
							<a href="index.html#Education">Education</a>
							<a href="index.html#Experience">Experience</a>
							<a href="Projects.html#DLProjects">Projects</a>
							<a href="index.html#Contact">Contact</a>
							<!-- <a href="./images/Resume-Website.pdf" download>Resume</a> -->
							<!-- <a>Resume</a> -->
						</nav>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<div class="inner">
							<h2>Menu</h2>
							<ul class="links">
								<li><a href="index.html">Home</a></li>
								<li><a href="#">About</a></li>
								<li><a href="index.html#Education">Education</a></li>
								<li><a href="#">Work Experience</a></li>
								<li><a href="#">Sign Up</a></li>
							</ul>
							<a href="#" class="close">Close</a>
						</div>
					</nav>

				<!-- Wrapper -->
					<section id="wrapper">
						<header>
							<div class="inner">
								<h2>Projects</h2>
								<!-- <p>Phasellus non pulvinar erat. Fusce tincidunt nisl eget ipsum.</p> -->
							</div>
						</header>

						

							




							<div class="wrapper alt style2">
								<nav id="DLProjects">
								<div class="inner">

									<h2 class="major">Robotics, Deep Learning and Computer Vision Projects</h2> 

									<h3 class="link">MMPUG - Multi-Modal Perception Uber Good</h3>
											<div style="padding:0px 50px 50px 50px;">
											
												<ul style="color:rgb(191, 210, 233) ;">
													<li>This project is a heterogenous high-speed multi-robot system used to rapidly map, navigate, search and rescue in unstructured environments. </li>
													<li>My Work is focused on development, testing, and deployment of SLAM and
														Perception framework on a heterogeneous fleet of robots comprising fast-moving RC cars and quadrupedal robot.</li>
														
														<li></span><a  href="https://www.ri.cmu.edu/project/mmpug-multi-model-perception-uber-good/">Project website</a></li>
													
														
													</ul>
													<article>
														<a class="image3"><img src="images/MMPUG.jpg" alt="" /></a>
														
														
														
														
														
														
													</article>	
												</div>




									<h3 class="link">Autonomous Drone Exploration</h3>
											<div style="padding:0px 50px 50px 50px;">
											
												<ul style="color:rgb(191, 210, 233) ;">
													<li>Research Focused on <b>Navigation and guidance for Autonomous Drone exploration</b> advised by <b>Prof. Sebastian Scherer.</b> </li>
														<li><b>Multi-view Wide-Angle State Estimation and Reconstruction for Autonomous Flight</b> to estimate the relative motion and
															create 360 depth maps to enable a drone to fly autonomously. </li>
															<li>Implemented ORB SLAM-3 visual odometry, for State Estimation and Reconstruction utilizing the drone’s Stereo fisheye
																multi-camera(6) feed. </li>
															<li>Worked on Visual Odometry for navigation as part of the Autonomy stack for autonomous drones .</li>
														
															
													
														
													</ul>
													<article>
														<a class="image3"><img src="images/ORBSlam-1.png" alt="" /></a>
														<a class="image3"><img src="images/ORBSlam-2.png" alt="" /></a>
														<a class="image3"><img src="images/ORBSlam-3.png" alt="" /></a>
														
														
														
														
														
													</article>	
												</div>
											<h3 class="link">SuperVLOAM - Super Visual LiDAR Odometry and mapping</h3>
											<div style="padding:0px 50px 50px 50px;">
											
												<ul style="color:rgb(191, 210, 233) ;">
													<li>State of the art technique for localizing and mapping an environment in real time using both camera and LIDAR sensors by fusing visual features and LIDAR data to estimate robots movements with higher precision.
													</li>
													<li>Implemented a robust real-time ROS-based framework for accurate trajectory estimation, 3D Mapping, and Localization by
														fusing stereo RGB image and LiDAR data using ICP in a non-linear optimization framework to achieve an ATE of 1.773m</li>
														<li>VLOAM used feature extractors such as ShiTomasi, ORB, BRIEF, FAST etc. The descriptor matching was done using Brute-force, L2-Norm and FLANN based matching. </li>
														<li>Augmented the feature extraction and matching algorithm with Super-Point descriptor and SuperGlue matching algorithm. </li>
														
															
													
														
													</ul>
													<article>
														<a class="image3"><img src="images/vloam1.png" alt="" /></a>
														<a class="image2"><img src="images/vloam2.png" alt="" /></a>
														<a class="image2"><img src="images/svloam top view.png" alt="" /></a>
														
														
														
														
														
													</article>	
												</div>
												<h3 class="link">Perception in Snow-Covered Environment</h3>
											<div style="padding:0px 50px 50px 50px;">
											
												<ul style="color:rgb(191, 210, 233) ;">
													<li>Modeled a comprehensive perception module for Progressive LiDAR adaptation for road detection in adverse Snow-Covered
														conditions through Sensor Fusion of Camera and LiDAR data on KITTI and Canadian Adverse Driving Conditions (CADC) dataset.</li>
														<li>This approach adapts LiDAR information into visual image-based road detection and improves detection performance.</li>
													
														
															
													
														
													</ul>
													<article>
														<a class="image3"><img src="images/PerceptionInSnow.png" alt="" /></a>
														<a class="image2"><img src="images/snow1.gif" alt="" /></a>
														<a class="image2"><img src="images/snow2.gif" alt="" /></a>
														<a class="image2"><img src="images/snow3.gif" alt="" /></a>
														<a class="image2"><img src="images/snow4.gif" alt="" /></a>
														
														
														
														
														
													</article>	
												</div>
												
												<h3 class="link">Super Deep SORT - Simple Online Real-time tracking</h3>
											<div style="padding:0px 50px 50px 50px;">
											
												<ul style="color:rgb(191, 210, 233) ;">
													<li>Combined the current State of the art multi-object tracking algorithm -<b> Deep SORT </b> with SuperGlue algorithm to enhance the object tracking in presence of Occlusion.</li>
														<li> Our implementation is robust to handle occlusion cases effectively and improves object tracking, matching and re-association of tracks by 2% copared to the baseline. </li>
													
													<li>The algorithm uses YOLOv7 for object detection. The object estimation and object matching uses three cost metrics.
														<ul>
														<li>SuperGlue Cost Metric</li>
														<li>Cosine Similarity Metric </li>
													<li> IOU cost metric using the kalman predicted bounding boxes</li> 
														</ul>
													</li>
													<li>Using the cost matrix, the matched tracks are succesfully re-assigned previous identity and unmatched tracks are assigned new ID's.</li>
													<li>This pipeline was extended to successfully track apples real time in an orchard - count and determine their location for a robot to perfectly
														pick them. </li>
															
													</ul>
													<article>
														
														<a class="image3"><img src="images/Deepsort-2.png" alt="" /></a>
														<a class="image2"><img src="images/Apple1.png" alt="" /></a>
														<a class="image2"><img src="images/Apple2.png" alt="" /></a>
														
														
														
														
														
													</article>	
												</div>

												<h3 class="link">HEXA-Human Demo Augmented Explorer and Achiever</h3>
											<div style="padding:0px 50px 50px 50px;">
											
												<ul style="color:rgb(191, 210, 233) ;">
													<li>Extended the SOTA LEXA Benchmark (Latent Explorer and Achiever) <a href="https://arxiv.org/abs/2110.09514">[Baseline Paper]</a> - a unified solution that learns a world model from image inputs and uses it to train an explorer and an achiever policy from imagined rollouts using human demonstrations.
														This project was advised by <b>Prof. Deepak Pathak</b>.
													</li>
													<li>The aim of the project was to augment the curiosity based exploration and achieving LEXA agent using
														human telop based demonstrations to explore and learn low skills(sub-tasks) to perform long-horizon tasks.</li>
														<li>This approach would help guide the exploration of LEXA around discovering goals that
															would be useful to perform the task carried out in the demonstration.We use the Franka Kitchen
															demonstration from D4RL to augment the exploration. We extended LEXA to long horizon
															tasks by extending exploration to start from different states.</li>
														
															
													
														
													</ul>
													<article>
														<video playsinline autoplay muted loop width="800">
															<source src="images/demonstration.mp4">
														</video>
														<a class="image3"><img src="images/LEXA.png" alt="" /></a>
														<a class="image2"><img src="images/hexa1.jpeg" alt="" /></a>
														<a class="image2"><img src="images/hexa2.jpeg" alt="" /></a>
														<a class="image2"><img src="images/hexa3.jpeg" alt="" /></a>
														<a class="image2"><img src="images/hexa4.jpeg" alt="" /></a>
														
														
														
														
														
														
													</article>	
												</div>

												<h3 class="link">DeepSpace</h3>
											<div style="padding:0px 50px 50px 50px;">
											
												<ul style="color:rgb(191, 210, 233) ;">
													<li>Led a team of 2 in the conceptualization and development of DeepSpace platform, a framework for adaptive real-time programming,
														control, monitoring, and task sequencing of a novel vision-enabled pick and place General Purpose Robotic Arm. </li>
														<li>Built a point cloud preprocessing pipeline with Intel Realsense, PCL, and gRPC for filtering and downsampling point cloud
															data for real-time streaming.</li>
														<li>Conceptualized and integrated the system architecture for sub-system communication and data storage.</li>
													<li>Integrated real-time Point-Cloud streaming from RealSense Camera using gRPC framework for real-time Monitoring, programming,
														and Task Sequencing in Unity.</li>
														<li>Wrote production-level, low-latency python and C# code for Communicating with ROS-based controller; extended existing
															SDK to support gRPC for a 6 DOF UFactory X-Arm.</li>
													
														
															
													
														
													</ul>
													<article>
														<a class="image3"><img src="images/VOA-Logo.png" alt="" /></a>
														<!-- <a class="image2"><img src="images/SpeechSteg-2.png" alt="" /></a> -->
														
														
														
														
														
													</article>	
												</div>

									<h3 class="link">Weakly Supervised Deep Detection Network</h3>
											<div style="padding:0px 50px 50px 50px;">
											
												<ul style="color:rgb(191, 210, 233) ;">
													
														<li>The project exploits the use of pre-trained CNN’s for Weakly supervised deep detection of image regions, performing simultaneous
															region selection and classification without using image-level annotations.</li>
															<li> A novel end-to-end method that classifies and predicts
															bounding boxes using AlexNet on PASCAL VOC data through the use of spatial pyramid pooling.</li>
															<li>This project is based on the following papers. 
																<a  href="https://www.di.ens.fr/~josef/publications/Oquab15.pdf">[Paper 1]</a>
																and <a  href="https://www.robots.ox.ac.uk/~vgg/publications/2016/Bilen16/bilen16.pdf">[Paper 2]</a>


															</li>
															
													
														
													</ul>
													<article>
														<a class="image2"><img src="images/WSDDN-1.png" alt="" /></a>
														<a class="image2"><img src="images/WSDDN-2.png" alt="" /></a>
														<a class="image2"><img src="images/WSDDN-3.png" alt="" /></a>
														<a class="image2"><img src="images/WSDDN-4.png" alt="" /></a>
														<a class="image2"><img src="images/WSDDN-5.png" alt="" /></a>
														<a class="image2"><img src="images/WSDDN-Logo.png" alt="" /></a>
														
														
														
														
													</article>	
												</div>
												<h3 class="link">Monte Carlo Localization - Particle Filter for Robot Localization</h3>
												<div style="padding:0px 50px 50px 50px;">
												
													<ul style="color:rgb(191, 210, 233) ;">
														<li>Implemented a Particle filter to localize a robot in an indoor environment.</li>	
														<li>Implemented motion model using odometer readings, sensor model using the LiDar Scan readings and ray tracing algorithm.</li>												
															
														</ul>
														<article>
															<video playsinline autoplay muted loop width="800">
																<source src="images/VID-20230224-WA0010.mp4">
															</video>
															
															
															
															
															
														</article>	
													</div>


												<h3 class="link">Visual Question Answering</h3>
												<div style="padding:0px 50px 50px 50px;">
												
													<ul style="color:rgb(191, 210, 233) ;">
														<li>Implmented a Multi-Modal Visual Question Answering model  using pre-trained <b> RoBERTa and TransformerNet</b> architecture.</li>
															<li>Incorporated self-attention and cross-attention to aid interactions between textual and visual features.</li>	
															<li>Achieved an accuracy of 67.62%</li>
														
															
														</ul>
														<article>
															<a class="image3"><img src="images/teaser.png" alt="" /></a>
															<a class="image3"><img src="images/transformer_vqa.jpg" alt="" /></a>
															
															
															
															
															
														</article>	
													</div>

												<h3 class="link">FACE CLASSIFICATION AND VERIFICATION USING CNN'S</h3>
											<div style="padding:0px 50px 50px 50px;">
											
												<ul style="color:rgb(191, 210, 233) ;">
													<li>Implemented <b>ResNet-34 and ResNet-50</b> from scratch for classification of VGGFace2 dataset.Utilized Triplet loss to increase the performance of face recognition.</li>
													<li>Implemented <b>ConvNext-Tiny</b> architecture for classification and verification.</li>
													<li>Experimented with Ensemble Methods to improve the accuracy. Achieved an Accuracy of 94.5%.</li>
													<li>Fine-tuned the model with triplet and center loss.</li>

														
													
														
															
													
														
													</ul>
													<article>
														<!-- <a class="image2"><img src="images/WSDDN-1.png" alt="" /></a> -->
														
														
														
														
														
													</article>	
												</div>



											<h3 class="link">Speech Steganography Using Deep Learning</h3>
											<div style="padding:0px 50px 50px 50px;">
											
												<ul style="color:rgb(191, 210, 233) ;">
													<li>Implemented a Deep learning network which explored the use of neural networks as steganographic functions for speech data.</li>
														<li> The objective was to conceal multiple messages in a single
														carrier using multiple decoders or a single conditional decoder such that it is unnoticeable by human listeners and that the decoded
														messages are highly intelligible.</li>
													
														
															
													
														
													</ul>
													<article>
														<a class="image2"><img src="images/SpeechSteg-1.png" alt="" /></a>
														<a class="image2"><img src="images/SpeechSteg-2.png" alt="" /></a>
														
														
														
														
														
													</article>	
												</div>
												
												<h3 class="link">Learning Strategies for
													Unsupervised Adaptation on Test Set</h3>
											<div style="padding:0px 50px 50px 50px;">
											
												<ul style="color:rgb(191, 210, 233) ;">
													<li>Research project on shift estimation of alignments in the test and train dataset to reduce the distribution mismatch which leads
														to poorer classification. </li>
														<li>Proposes an affine transformation to help learn better fit to the test data</li>
														<li>Learning strategies compared against <b>Audio Speech Transformer (AST)</b> model as baseline using Audioset dataset.</li>
													
														
															
													
														
													</ul>
													<article>
														<a class="image3"><img src="images/ast.png" alt="" /></a>
														
														
														
														
														
													</article>	
												</div>




												<h3 class="link">ATTENTION-BASED END-TO-END SPEECH-TO-TEXT DEEP NEURAL NETWORK</h3>
											<div style="padding:0px 50px 50px 50px;">
											
												<ul style="color:rgb(191, 210, 233) ;">
													<li>Implemented a multi-head attention based Speech to Text Deep Neural Network using <b> ”Listen, Attend and Spell”</b> paper as the baseline model.</li>
													<li>Pre-processed speech data and transcripts for neural network input and designed depthwise convolution layer for feature extraction and embedding layers.</li>
														<li>Results: Levenshtein distance (8.7); reached an A score in the Kaggle competition.</li>
															
													
														
													</ul>
													<article>
														<a class="image2"><img src="images/Attention-LAS.png" alt="" /></a>
														
														
														
														
														
													</article>	
												</div>

												
												

												
												<h3 class="link">UTTERANCE TO PHONEME MAPPING USING RNN'S</h3>
											<div style="padding:0px 50px 50px 50px;">
											
												<ul style="color:rgb(191, 210, 233) ;">
													
														
															
													
														
													</ul>
													<article>
														<!-- <a class="image2"><img src="images/WSDDN-1.png" alt="" /></a> -->
														
														
														
														
														
													</article>	
												</div>
												<h3 class="link">PHONEME LEVEL CLASSIFICATION OF SPEECH</h3>
											<div style="padding:0px 50px 50px 50px;">
											
												<ul style="color:rgb(191, 210, 233) ;">
													
														
															
													
														
													</ul>
													<article>
														<!-- <a class="image2"><img src="images/WSDDN-1.png" alt="" /></a> -->
														
														
														
														
														
													</article>	
												</div>
									
										

									

								</div>
							</nav>
							</div>

							<!-- Content -->
							<div class="wrapper style1">
								<nav id="SimulatorProjects">
								<div class="inner">

									<h2 class="major">Simulator Projects</h2> 

									<h3 class="link">Virtual Reality Based Airport Rescue and Fire-Fighting Simulator</h3>
											<div style="padding:0px 50px  50px 50px ;">
											
												<ul style="color:rgb(191, 210, 233) ;">
														<li>Developed and Deployed a Virtual Reality based Simulator at an airport to deliver realistic ARFF operation training by simulating
															vehicle behavior, accident prone scenarios and authentic controls within three months.</li>
															<li>The objectives of this project was to reducing both safety risks and problems caused due to ground operations in the airside.</li>
															<li>The Simulator dualed as a Airside Driving Permit test simulator as a pre-test to the airside drivers before getting a driver permit. This test is conducted every six-months as a recap of airside driving rules.</li>
														<li>The simulator includes simulation of radio management and communication with Air Traffic Control and all procedures so that the trainee can make the right decisions in specific situations such as a Fire hazard.</li>
														<li>The trainee drives on a Motion Platform which makes him feel and react exactly as he would do in real life. The trainee is tested on several operational scenarios under an instructor’s supervision.</li>
															<li>The Simulator was developed using <b>HTC VIVE</b> Device to provide a Virtual Scene of the entire airport. Leap Motion device was used for real hand tracking of the trainee. </li>
															<li>A <b>3 DOF Motion platform</b> along with Logitech steering and Gear system were used to control the driving.</li>
															<li>User Inputs and Data Acquisition system were developed and integrated with the
																Unity 3D application to simulate the real cabin controls .</li>
														
													</ul>
													<article>
														<a class="image2"><img src="images/Arff-1.png" alt="" /></a>
														<a class="image2"><img src="images/Arff-2.png" alt="" /></a>
														<a class="image2"><img src="images/Arff-3.png" alt="" /></a>
														<a class="image2"><img src="images/Arff-4.png" alt="" /></a>
														<a class="image2"><img src="images/Arff-5.png" alt="" /></a>
														<a class="image2"><img src="images/Arff-6.png" alt="" /></a>
														<a class="image2"><img src="images/Arff-7.png" alt="" /></a>
														<a class="image2"><img src="images/Arff-8.png" alt="" /></a>
														<a class="image2"><img src="images/Arff-9.png" alt="" /></a>
														<a class="image2"><img src="images/Arff-10.png" alt="" /></a>
														
													</article>	
												</div>

												<h3 class="link">Automotive Bus Driving Simulator</h3>
											<div style="padding:0px 50px 50px 50px;">
											
												<ul style="color:rgb(191, 210, 233) ;">
													<li>Developed a 3DOF motion-based automotive simulator for training and familiarization of driving heavy vehicles which incorporates
													traffic incidents and vehicle malfunctions.</li>
													<li> contributions involved interfacing the actual steering and gear control feedback systems, traffic incidents and vehicle
													malfunctions modules.</li>
														
													</ul>
													<article>
														<a class="image2"><img src="images/DS-1.png" alt="" /></a>
														<a class="image2"><img src="images/DS-2.png" alt="" /></a>
														<a class="image2"><img src="images/DS-3.png" alt="" /></a>
														<a class="image2"><img src="images/DS-4.png" alt="" /></a>
														<a class="image2"><img src="images/DS-5.png" alt="" /></a>
														
														
													</article>	
												</div>

												<h3 class="link">Monorail Locomotive and Cockpit Training Simulator</h3>
											<div style="padding:0px 50px 50px 50px;">
											
												<ul style="color:rgb(191, 210, 233) ;">
													<li>Design and Development of a Monorail replica training station to produce an accurate simulation of real traffic conditions,
														protection systems, cab signaling, breakdown protocol training, eco driving etc. Implemented Cockpit controls, traction/brake,
														hydraulic, HMIs, door operation training modules.</li>
													
														
													</ul>
													<article>
														<a class="image2"><img src="images/MM-4.png" alt="" /></a>
														<a class="image2"><img src="images/MM-3.png" alt="" /></a>
														
														
														
													</article>	
												</div>

												<h3 class="link">Augmented Reality based Emergency Medical Training Simulation</h3>
											<div style="padding:0px 50px 50px 50px;">
											
												<ul style="color:rgb(191, 210, 233) ;">
													<li>This work presents a AR based medical training prototype designed to train medical practitioner’s on performing Endotracheal Intubations also known as Laryngoscopy. </li>
														<li>The system 
														 accomplishes this task with the help of AR simulation. The system will allow paramedics,
														pre-hospital personnel, and students to practice their skills without touching a real patient and
														will provide them with the visual feedback they could not otherwise obtain.
														
														</li>
														<li>This simulator was developed in an effort to improve airway management training and respiratory system prognostics Utilizing a 3D human dummy model
															with detailed internal organs of the upper thorax combined with 3D AR visualization of the
															airway anatomy and three kinds of Laryngoscopes, paramedics will be able to obtain a visual
															and tactile sense of proper ETI procedure.</li>
															<li>The system consists of a <b>Magic Leap Head Mount Device</b> (HMD) where the trainee sees a virtual 3D model of the human
																dummy and the User Interface superimposed onto the real world as shown below. </li>
													
														
													</ul>
													<article>
														<a class="image2"><img src="images/EMS-1.png" alt="" /></a>
														<a class="image2"><img src="images/EMS-2.png" alt="" /></a>
														<a class="image2"><img src="images/EMS-3.png" alt="" /></a>
														<a class="image2"><img src="images/EMS-4.png" alt="" /></a>
														<a class="image2"><img src="images/EMS-5.png" alt="" /></a>
														
														
														
														
													</article>	
												</div>
												
									
									
										

									

								</div>
							</nav>
							</div>
							<!-- <div class="wrapper">
								<nav id="CVProjects">
								<div class="inner">

									<h2 class="major">COMPUTER VISION PROJECTS</h2> 
									
										

									

								</div>
							</nav>
							</div> -->

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>